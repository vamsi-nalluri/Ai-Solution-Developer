{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864f1a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('abcnews-date-text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1879d",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3e9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "Feature Names (BoW):\n",
      " ['000' '03' '0388' ... 'zurich' 'zvonareva' 'zyl']\n",
      "=============================================================================================================================\n",
      "Bag-of-Words Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# Bag-of-Words (BoW) using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "x_bow = vectorizer.fit_transform(data['headline_text'])\n",
    "feature_names_bow = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(\"\\nFeature Names (BoW):\\n\", feature_names_bow)\n",
    "print(\"=\"*125)\n",
    "print(\"Bag-of-Words Matrix:\\n\", x_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692da59",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15733e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "Feature Names (TF-IDF):\n",
      " ['000' '03' '0388' ... 'zurich' 'zvonareva' 'zyl']\n",
      "=============================================================================================================================\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# # TF-IDF using TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "x_tfidf = tfidf_vectorizer.fit_transform(data['headline_text'])\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(\"\\nFeature Names (TF-IDF):\\n\", feature_names_tfidf)\n",
    "print(\"=\"*125)\n",
    "print(\"\\nTF-IDF Matrix:\\n\", x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1ffd4",
   "metadata": {},
   "source": [
    "# N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0757e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "Generated 1-grams:\n",
      " [('aba',), ('decides',), ('against',), ('community',), ('broadcasting',), ('licence',)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# N-grams\n",
    "n = 1\n",
    "text = data['headline_text'].iloc[0]  \n",
    "tokens = nltk.word_tokenize(text)\n",
    "unigrams = list(ngrams(tokens, n))\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(f\"\\nGenerated {n}-grams:\\n\", unigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562be55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "Generated 2-grams:\n",
      " [('aba', 'decides'), ('decides', 'against'), ('against', 'community'), ('community', 'broadcasting'), ('broadcasting', 'licence')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# N-grams\n",
    "n = 2\n",
    "text = data['headline_text'].iloc[0]  \n",
    "tokens = nltk.word_tokenize(text)\n",
    "bigrams = list(ngrams(tokens, n))\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(f\"\\nGenerated {n}-grams:\\n\", bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7ecf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "Generated 3-grams:\n",
      " [('aba', 'decides', 'against'), ('decides', 'against', 'community'), ('against', 'community', 'broadcasting'), ('community', 'broadcasting', 'licence')]\n"
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "n = 3\n",
    "text = data['headline_text'].iloc[0]  \n",
    "tokens = nltk.word_tokenize(text)\n",
    "trigrams = list(ngrams(tokens, n))\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(f\"\\nGenerated {n}-grams:\\n\", trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783d319",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38017039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "\n",
      "One-Hot Encoded Matrix:\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-Hot Encoding\n",
    "text_for_onehot = ['search for missing angler called off', 'two badly burned in ballarat car explosion',\n",
    "                   'govts closer to ship sinking agreement', 'howard meets megawati at apec']\n",
    "tokens_onehot = [word for sent in text_for_onehot for word in sent.lower().split()]\n",
    "vocabulary_onehot = list(set(tokens_onehot))\n",
    "\n",
    "encoder = OneHotEncoder(categories=[vocabulary_onehot], sparse=False)\n",
    "one_hot_encoded = []\n",
    "\n",
    "for sent in text_for_onehot:\n",
    "    sent_encoded = []\n",
    "    for word in sent.lower().split():\n",
    "        word_index = vocabulary_onehot.index(word)\n",
    "        word_vector = np.zeros(len(vocabulary_onehot))\n",
    "        word_vector[word_index] = 1\n",
    "        sent_encoded.append(word_vector)\n",
    "    one_hot_encoded.append(sent_encoded)\n",
    "    \n",
    "    \n",
    "print(\"=\"*125)\n",
    "print(\"\\nOne-Hot Encoded Matrix:\")\n",
    "for sent in one_hot_encoded:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7e3b5",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b332977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Vector for 'harrased':\n",
      "\n",
      "\t [-1.25479409e-02  1.43873235e-02  6.83394819e-03 -8.43549613e-03\n",
      " -2.88833049e-04 -8.27173237e-03 -4.06600484e-05  2.17473470e-02\n",
      " -3.28645017e-03 -2.42309156e-03  1.51883741e-03 -1.73710417e-02\n",
      "  2.55823089e-03  4.57047764e-03  4.97877086e-03 -4.88871150e-03\n",
      "  9.53605678e-03 -9.14497126e-04 -6.80655008e-03 -1.51521424e-02\n",
      " -2.78057624e-03 -9.81673412e-03 -5.70116378e-03 -4.29448020e-03\n",
      " -1.26733258e-03  8.96376371e-03 -7.74835935e-03 -7.73743680e-03\n",
      "  1.01984502e-03  1.05377240e-02  1.23139226e-03  5.66085288e-03\n",
      " -2.35751766e-04 -3.42668639e-03 -8.23798683e-03  4.21335874e-03\n",
      "  5.64261898e-03 -2.94797472e-03 -1.57702044e-02 -1.60490796e-02\n",
      "  4.23522986e-04 -9.47563630e-03 -1.47837764e-02 -3.59171862e-03\n",
      " -9.47497902e-04 -3.27237276e-03  7.07197294e-04 -6.35047443e-03\n",
      " -2.07854202e-03  1.34857814e-03  1.13158906e-02 -9.88053903e-03\n",
      " -1.76063110e-03  9.77788214e-03 -4.16142540e-03 -1.83997001e-03\n",
      "  5.19989012e-03 -1.24334581e-02 -9.29252710e-03  3.54603818e-03\n",
      "  2.08190922e-03  2.18188157e-03  6.02377520e-04 -1.33046396e-02\n",
      "  1.22076820e-03  1.11041991e-02  4.93712630e-03  1.32372475e-03\n",
      " -4.56567667e-03  1.02910493e-02 -7.19337584e-03  1.05001787e-02\n",
      " -6.29354035e-05 -5.02069294e-03  2.48179049e-03  1.16827181e-02\n",
      "  8.61372240e-03  9.89587698e-03  3.45293409e-03 -8.71163735e-04\n",
      " -5.74400555e-03 -1.61192007e-03 -8.46926123e-03  1.13289207e-02\n",
      " -2.37930636e-03  7.92433601e-03  2.26745312e-03  6.39850320e-03\n",
      "  1.09466221e-02  5.30273945e-04  7.09845591e-03  1.56986713e-02\n",
      "  1.12697482e-02 -4.27022064e-03  1.09103527e-02 -1.91247556e-03\n",
      "  1.31550385e-02 -2.13764757e-02  5.48914913e-03  1.11068459e-02]\n",
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "Vector for 'ambulance':\n",
      "\n",
      "\t [-0.18396552  0.18561321  0.01191194  0.0872386  -0.03318578 -0.40262488\n",
      "  0.10207697  0.5105466  -0.27546406 -0.18403116  0.04031704 -0.36550334\n",
      " -0.03877862  0.14416553  0.07344842 -0.23895825  0.12896103 -0.16788357\n",
      " -0.14629748 -0.50021553  0.05516215  0.11783431  0.2751067  -0.07440807\n",
      " -0.00852205  0.06247202 -0.24141626 -0.03078476 -0.16857147  0.10180909\n",
      "  0.2435874  -0.10176274  0.01108722 -0.25719044  0.00696832  0.33115816\n",
      "  0.06036682 -0.26760927 -0.19268014 -0.36376604 -0.0261673  -0.2326847\n",
      " -0.35327983  0.02515775  0.20396115 -0.00724769 -0.18679358  0.04902409\n",
      "  0.16815035  0.17959887  0.12607461 -0.17771114 -0.08712149 -0.00232668\n",
      " -0.17579295  0.08358357  0.18818241 -0.04241814 -0.11588099  0.11599906\n",
      "  0.09013962 -0.00611569  0.0469737   0.02836024 -0.22011779  0.3050323\n",
      "  0.02245727  0.29792827 -0.37607044  0.25453648 -0.12140889  0.37023032\n",
      "  0.28156418  0.01128888  0.2043996   0.24364872  0.01575886 -0.00452375\n",
      " -0.14978863 -0.08913343 -0.15269761 -0.1365363  -0.02066488  0.19067828\n",
      " -0.11624161 -0.00874147  0.00443796  0.10760648  0.21785596  0.18179776\n",
      "  0.09888906  0.14395538  0.1408745   0.03166087  0.574168    0.25349662\n",
      "  0.17432046 -0.3411457   0.09308794  0.05469221]\n",
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "Similarity between 'Chemical' and 'arrested':\t0.8544438481330872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Tokenize the headlines\n",
    "tokenized_headlines = [word_tokenize(sentence.lower()) for sentence in data['headline_text']]\n",
    "\n",
    "# Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=tokenized_headlines, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec_model\")\n",
    "\n",
    "# Example usage of Word2Vec model\n",
    "vector_harrased = word2vec_model.wv['harassed']\n",
    "vector_ambulance = word2vec_model.wv['ambulance']\n",
    "similarity = word2vec_model.wv.similarity('chemical', 'arrested')\n",
    "\n",
    "print(\"=\"*125)\n",
    "print(f\"Vector for 'harrased':\\n\\n\\t {vector_harrased}\\n\\n\")\n",
    "print('=' * 125)\n",
    "print(f\"Vector for 'ambulance':\\n\\n\\t {vector_ambulance}\\n\\n\")\n",
    "print('=' * 125)\n",
    "print(f\"Similarity between 'Chemical' and 'arrested':\\t{similarity}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd5f9",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef6b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Tagged data:\n",
      "\n",
      " [TaggedDocument(words=['death', 'toll', 'continues', 'to', 'climb', 'in', 'south', 'korean', 'subway'], tags=['0']), TaggedDocument(words=['funds', 'to', 'go', 'to', 'cadell', 'upgrade'], tags=['1']), TaggedDocument(words=['man', 'charged', 'over', 'cooma', 'murder'], tags=['2'])]\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "vector for 'man charged over cooma murder':\n",
      "\n",
      "[ 3.02760839e-03  3.62603250e-03  4.74082446e-03 -7.72489177e-04\n",
      " -7.22767552e-04  4.34913952e-03  1.84735365e-03  2.54158303e-03\n",
      " -3.45497672e-03 -2.46411818e-03  1.21556615e-04  3.97946872e-03\n",
      "  3.33664147e-03  6.42365660e-04 -2.89805001e-03  3.55820230e-04\n",
      "  4.91429633e-03  4.82639298e-03  1.42807909e-03 -2.81690550e-03\n",
      "  3.78943840e-03  1.54533668e-03  4.84450208e-03  2.68553663e-03\n",
      "  3.80434655e-03  7.92035658e-04 -6.26541616e-04 -2.73808814e-03\n",
      " -9.25975735e-04 -3.97102861e-03  7.59969000e-04  1.60412851e-03\n",
      "  3.47616011e-03  4.06145072e-03 -2.20453949e-03 -9.78054362e-04\n",
      "  4.05522529e-03 -3.31065315e-03  1.36917655e-03  4.17039124e-03\n",
      "  9.22242762e-04 -2.01136735e-03  1.91533612e-03  4.37323749e-03\n",
      " -1.53069777e-04 -6.81468227e-04 -1.02634344e-03 -1.46734563e-03\n",
      " -3.32460972e-03  1.56983535e-03 -4.10939858e-04  4.42365184e-03\n",
      " -3.26901022e-03 -4.69925348e-03  4.49286448e-03 -2.10144138e-03\n",
      "  7.72479514e-04 -1.64598820e-03  4.19911137e-03 -4.62922268e-03\n",
      " -1.40136620e-03 -2.74997880e-03 -4.55157133e-03 -4.65241726e-03\n",
      "  3.69911455e-03  4.91537200e-03  4.85569052e-03 -4.40513855e-03\n",
      "  1.98865822e-03 -2.97147385e-03 -3.66384699e-03 -4.85352613e-03\n",
      " -1.20546424e-03 -1.53778750e-03 -4.84415190e-03 -4.05769097e-03\n",
      " -1.46999722e-03 -1.25977932e-03 -5.30099642e-05 -4.97672381e-03\n",
      "  1.39377872e-03  4.59032925e-03 -1.81103894e-03 -3.52488569e-04\n",
      " -3.06502008e-03 -8.34066188e-04  4.19323286e-03  8.09102028e-04\n",
      "  2.89529766e-04 -3.12799122e-03 -1.26356015e-03 -3.03539191e-03\n",
      " -1.33863697e-03 -7.21756427e-04  3.86524759e-03  1.03482790e-03\n",
      "  1.55213801e-03 -4.90231486e-03  4.94974153e-03  2.79679056e-03]\n",
      "=============================================================================================================================\n",
      "Most similar document:\n",
      "\n",
      "[('0', 0.057277288287878036), ('2', -0.053867630660533905), ('1', -0.1713951975107193)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "#sample text\n",
    "documents=['death toll continues to climb in south korean subway',\n",
    "           'funds to go to cadell upgrade',\n",
    "           'man charged over cooma murder']\n",
    "\n",
    "#Tokenize & tag documents\n",
    "tagged_data=[TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                            tags=[str(i)]) for i,doc in enumerate(documents)]\n",
    "print('=' * 125)\n",
    "print(\"Tagged data:\\n\\n\",tagged_data)\n",
    "print('=' * 125)\n",
    "#Train Doc2vec model\n",
    "model=Doc2Vec(vector_size=100,window=2,min_count=1,workers=5,epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "\n",
    "\n",
    "vector_doc_1=model.infer_vector(word_tokenize(\"man charged over cooma murder\"))\n",
    "                                              \n",
    "#find the most similar document\n",
    "similar_doc=model.dv.most_similar(positive=[vector_doc_1])\n",
    "print('=' * 125)\n",
    "print(f\"vector for 'man charged over cooma murder':\\n\\n{vector_doc_1}\")\n",
    "print('=' * 125)\n",
    "print(f\"Most similar document:\\n\\n{similar_doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048244a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
